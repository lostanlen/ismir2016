@article{Abdel-Hamid2014,
  title={Convolutional neural networks for speech recognition},
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal={Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  volume={22},
  number={10},
  pages={1533--1545},
  year={2014},
  publisher={IEEE}
}

@inproceedings{Anden2015,
abstract = {We introduce the joint time-frequency scattering transform, a time shift invariant descriptor of time-frequency structure for audio classification. It is obtained by applying a two-dimensional wavelet transform in time and log-frequency to a time-frequency wavelet scalogram. We show that this descriptor successfully characterizes complex time-frequency phenomena such as time-varying filters and frequency modulated excitations. State-of-the-art results are achieved for signal reconstruction and phone segment classification on the TIMIT dataset.},
archivePrefix = {arXiv},
arxivId = {1512.02125},
author = {And{\'{e}}n, Joakim and Lostanlen, Vincent and Mallat, St{\'{e}}phane},
booktitle = {Machine Learning for Signal Processing},
doi = {10.1109/MLSP.2015.7324385},
eprint = {1512.02125},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/e8ca21f1ef9b9dbe3f51eccf3d5afd24d08df1c3.pdf:pdf},
isbn = {9781467374545},
title = {{Joint Time-Frequency Scattering for Audio Classification}},
url = {http://arxiv.org/abs/1512.02125
http://dx.doi.org/10.1109/MLSP.2015.7324385},
year = {2015}
}

@inproceedings{Bittner2014,
abstract = {We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
booktitle = {International Society for Music Information Retrieval Conference},
title = {{MedleyDB: a multitrack dataset for annotation-intensive MIR research}},
url = {http://marl.smusic.nyu.edu/medleydb{\_}webfiles/bittner{\_}medleydb{\_}ismir2014.pdf},
year = {2014}
}

@misc{Chollet2015,
author = {Fran\c{c}ois Chollet},
title = {Keras: a Deep Learning library for Theano and TensorFlow},
year = {2015},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {https://github.com/fchollet/keras},
commit = {5bcac37}
}

@inproceedings{Eronen2000,
  title={Musical instrument recognition using cepstral coefficients and temporal features},
  author={Eronen, Antti and Klapuri, Anssi},
  booktitle={Acoustics, Speech, and Signal Processing, 2000. ICASSP'00. Proceedings. 2000 IEEE International Conference on},
  volume={2},
  pages={II753--II756},
  year={2000},
  organization={IEEE}
}

@inproceedings{Hamel2012,
abstract = {Low-level aspects of music audio such as timbre, loud- ness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and re- quire a better representation of time dynamics. For var- ious music information retrieval tasks, one would bene?t from modelling both low and high level aspects in a uni- ?ed feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale fea- tures. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for auto- matic tag annotation.},
author = {Hamel, Philippe and Bengio, Yoshua and Eck, Douglas},
booktitle = {ISMIR},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/18e4b042f9134a9d963ecc9fa0e17703e06572cc.pdf:pdf},
isbn = {9789727521449},
pages = {553--558},
title = {{Building musically-relevant audio features through multiple timescale representations}},
url = {http://ismir2012.ismir.net/event/papers/553{\_}ISMIR{\_}2012.pdf},
year = {2012}
}

@inproceedings{Humphrey2012,
  title={Learning a robust tonnetz-space transform for automatic chord recognition},
  author={Humphrey, Eric J and Cho, Taemin and Bello, Juan P},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on},
  pages={453--456},
  year={2012},
  organization={IEEE}
}

@article{Joder2009,
abstract = {Nowadays, it appears essential to design automatic indexing tools which provide meaningful and efficient means to describe the musical audio content. There is in fact a growing interest for music information retrieval (MIR) applications amongst which the most popular are related to music similarity retrieval, artist identification, musical genre or instrument recognition. Current MIR-related classification systems usually do not take into account the mid-term temporal properties of the signal (over several frames) and lie on the assumption that the observations of the features in different frames are statistically independent. The aim of this paper is to demonstrate the usefulness of the information carried by the evolution of these characteristics over time. To that purpose, we propose a number of methods for early and late temporal integration and provide an in-depth experimental study on their interest for the task of musical instrument recognition on solo musical phrases. In particular, the impact of the time horizon over which the temporal integration is performed will be assessed both for fixed and variable frame length analysis. Also, a number of proposed alignment kernels will be used for late temporal integration. For all experiments, the results are compared to a state of the art musical instrument recognition system.},
author = {Joder, Cyril and Essid, Slim and Richard, Ga{\"{e}}l},
doi = {10.1109/TASL.2008.2007613},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Alignment kernels,Audio classification,Music information retrieval (MIR),Musical instrument recognition,Support vector machine (SVM),Temporal feature integration},
number = {1},
pages = {174--186},
title = {Temporal integration for audio classification with application to musical instrument classification},
url = {http://biblio.telecom-paristech.fr/cgi-bin/download.cgi?id=8585},
volume = {17},
year = {2009}
}

@article{Kereliuk2015,
abstract = {An adversary is essentially an algorithm intent on making a classification system perform in some particular way given an input, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, which exploits the parameters of the system to find the minimal perturbation of the input image such that the network misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the input to the systems is magnitude spectral frames, which requires special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different deep architectures, we find that this adversary is very effective in defeating the resulting systems. We find the convolutional networks are more robust, however, compared with systems based on a majority vote over individually classified audio frames. Furthermore, we integrate the adversary into the training of new deep systems, but do not find that this improves their resilience against the same adversary.},
archivePrefix = {arXiv},
arxivId = {1507.04761},
author = {Kereliuk, Corey and Sturm, Bob L. and Larsen, Jan},
doi = {10.1109/TMM.2015.2478068},
eprint = {1507.04761},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/01005e6cff03ae253c088741b53ffe4a7280450e.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {AEA-MIR content-based processing and music information retrieval,deep learning},
number = {11},
pages = {2059--2071},
title = {{Deep Learning and Music Adversaries}},
url = {http://www2.imm.dtu.dk/pubdb/views/edoc{\_}download.php/6904/pdf/imm6904.pdf},
volume = {17},
year = {2015}
}

@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/cf5cf758f1b00214d70e34f8a57813557dc03e17.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
url = {http://arxiv.org/pdf/1412.6980v8.pdf},
year = {2015}
}

@book{Krumhansl2001,
  title={Cognitive Foundations of Musical Pitch},
  author={Krumhansl, Carol L.},
  year={2001},
  publisher={Oxford University Press}
}

@article{Li2015,
abstract = {Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these "shallow" architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise. In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {1511.05520},
author = {Li, Peter and Qian, Jiyuan and Wang, Tian},
eprint = {1511.05520},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/fedeeda2182eef200a4ac9af9caf95708bd0ecca.pdf:pdf},
journal = {arXiv preprint},
title = {{Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1511.05520},
year = {2015}
}

@misc{McFee2015-librosa,
  author       = {Brian McFee and
                  Matt McVicar and
                  Colin Raffel and
                  Dawen Liang and
                  Oriol Nieto and
                  Eric Battenberg and
                  Josh Moore and
                  Dan Ellis and
                  Ryuichi Yamamoto and
                  Rachel Bittner and
                  Douglas Repetto and
                  Petr Viktorin and
                  Jo\犸棋扉疱俞铘矬犷龄蜷犷蕊祜鲠豉糸綮扉怛矬岷爱串碑阱铒滹卑挡副孱镤锂备扯过盹铘镢衄遽舶钡滹卑挡副孱镤锂巢惫除躜梏麴函澍滹楫矧绡卑挡副孱镤锂巢惫除篱铕蝻沐邃轭珞豌驽宀氨淡眭溽狨翳矧豌驽瀣买獒犷弱眇栩妁膨殂犷洛祆铿术犷旋怙镫糸綮捎蜕引骈戾赫箦蝮鲮矬翎畀涕怛狎琉痨殂狒轱吁痧矧舣湾钿屐妁腻箅麸鸠娘黝祜徜邃繁後垛岬版创愀卑栋溻斗惴汊渤蜂层惴测灞姹痄婧痄纨疳珏泊腑驳待糸綮箫骠麽蝈骝犴鬻矧骘眭箝汜溽翎狨珥孱翎糸镱躜梏麴蠛忭沔邋玳翳踱轱疳疱蝮轶黹虿氨蝶苓狨珥孱翎糸镱痄纨遽舶钡泪螋殂戾五黥镱舶辈徕篝蜥泗需箝镬镧殂犰弼殇孱沐篚珑弩趔翳狒箫躅镱箦溴翦泗轱轭翳狨溟麸蝙簌篝屙磲忮疱蜴矧礤怡箴邈獒扉邃铄躜镱狍遽蜢狍翳泔汨戾狎铛沆艴螽畜汨镝泔躞糸弼殇孱沐箬秣翳狒翳箫躅镱箦汜忮轫痫螋犷骘翳蝈泔珙轸轱镦眭箝汜箫躅潴儒蝈翳箫躅镱箦轶躞邃轭轶镬狒轱麸骘蝽麸铄溴筱蜷痿矧骘眭箝汜轭篝蝓礤铘沆狍箝骈汜糸镱翎箅澡翎箅轭鲲祧弩舶傅轶镬狒邃眭箝汜麸铄骝镯翳豌情祆溽翎箦徙蝻篌骈鲥轭篝蝓礤铘汜翦顼蜷弩铄躜犰禊轭箴轵邃麸铄溴筱蜷痿矧轶泸遽翦躞轭盹溴镦翳狨溟麸蝙簌篝屙蝈箴镱箦麸箫躅镱箦舢玑眄狒镱骈祠弪忉铍犷箴殡轭镱箦溴翦泗矧蟋怩殪骝镯澌钺黹簌钺痼弩犷戾犭轭翦珧狒瀛犷洵骈蝈铄躜镱蟋泸遽翦疳蜥祆屐箴殡趄衢铙翳狒屙痂狍辁翳箫躅镱箦舢澡弩狎泔溴狍溴筱蜷痿矧汜祆邃翳镱箦骈铉弪痱轭舢渺狍箝骈汜糸镱躞弩糸礤滹磲轭铄躜犰铄赭矧氍翳邈栾篝狒铄赭矧氘义驽蝈钽篝蜥翦玳弩忉箦躔镱礤飙骝羼蹂钽沐痼趄犰泔彐骈汩孱趔弼犰踽翦彘翳弪秭弪翳麒镬麸铄矧镱禊漉蜷铉翳箫躅镱箦衄痱秭殇泔铘屮麸翳礤翳镤渺狍箝骈汜糸镱篚沣弩蜥翦骘翳铄躜犰禊轭箴轵邃礤翳镤狎狎秕钿返堀澡沐痼趄犰礤翳镤疱蜴矧忮赭邋烦堀犷范堀契螋桢翦篝轭鏖翳麸铄骝镯翳娠麽蜕泔祆邈糸镱箬秣翳狒翳铄躜犰禊轭箴轵邃礤翳镤轶泔铙殇弪徕禊盹蝈蝻怩篝麒孱翦篝邃鏖翳溽翎骝镯犷躅蝈灬翦溽翎箦舢狨翳矧五黥镱烷汨徨十犷禹轸璎体箪殄赢滹卑北脖碑捶胺党谍骈戾赫箦蝮鲮矬翎畀涕怛狎琉痨殂狒轱吁痧矧舣湾钿屐妁腻箅麸鸠娘黝祜徜邃宥锋靛扳舵丰搞垫洞瑰蹭膊獯饭悴岬惚蹭存忮痄婧痄纨轶箢鞍氨垂抖觑躜钺澡曙躜钺镦翳零秕篝殂犰语汩弭镦另弪殂猃铛礅弪洱疳珏捶傅痦殇膊繁补蛋糸綮铄躜犰禊轭箴轵邃眭箝汜轭篝蝓礤铘沆狍箝骈汜糸镱簌篝屙忉箦躔镱翳箫躅镱箦酏躜梏麴函潴疳沐篝轵徙蹼忾趔趄遽懑备钩脖饭动悲粕瘟帖苓柿影按犯诞痄纨鲲祯礤背饼遽舶辈篱铕蝻沐邃轭珞鱼桁豸弪舶贝糸綮褰身痱秭邃眭箝汜镱箦溴翦泗轱鏖翳泔铞镬豸轱钺铄躜犰铄赭矧塍狨翳矧禁鱼桁堍觚翦颥梳犷嘛汶渝忉篝獒铨怙镫糸綮褰零秕篝殂蟋羽邋汨犷娱珙犰序镢弩箝铉ㄉ昧佑些舶贝膳排深翦蝾狒轱钺蔑铈弪孱沐镱疳珏蠼豆饭豆赋遽蚪舶贝矧玑铋狒轱罱膳排泪螋殂戾侦祢殂璨氨船狨翳矧侦祢殂璎酸蝈犷鱼桁堍觚翦颥梳犷球殪飕澡镯狍骈戾赫箦蝮鲮矬翎畀涕怛狎琉痨殂狒轱吁痧矧舣湾钿屐妁腻箅麸鸠娘黝祜徜邃掣背赴骀徨獯洞倍浔村掣膊炒构驳夺邃吵忉稿痄婧痄纨觑躜钺序镢镦翳钡翳深翦蝾狒轱钺语汩弭骘王箝深骘蝽狒轱义趄殄鲠蔑铈弪孱沐疳珏幢翻床昌糸綮嘛躅溽蝙腻翦泗轱轭王箝郁蝓泗躜令犰箝阵轭蔑铞镬豸轱钺五躜犰五赭矧塍躜梏麴函鼢鳟镦衢狒jan.schlueter/pubs/2014{\_}ismir.pdf},
year = {2014}
}

